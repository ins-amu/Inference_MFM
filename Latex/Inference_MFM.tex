\documentclass[preprint,11pt,authoryear]{elsarticle}


\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{cases}
\usepackage{color}
\usepackage{colortbl}
\usepackage{tabularx}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{subfigure}
\usepackage{epsfig}
\usepackage{epstopdf}
\usepackage[labelfont=bf]{caption}
\usepackage{lineno}
\usepackage{lineno,hyperref}
\usepackage{changes}
\usepackage{siunitx}
\usepackage{listings}
\usepackage{tikz}
\usepackage{type1cm}
\usepackage[latin1]{inputenc}
%\usepackage{easyReview}
\usepackage{graphicx,dblfloatfix}
\usepackage[toc]{appendix}
\usepackage{xr}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps,pgfshade}
\usepackage{lipsum}
\usepackage{usebib}
\usepackage{filecontents}
\usepackage{multicol}
\usepackage{epsfig}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{makecell}
\captionsetup{labelsep=period}
\usepackage{verbatim}
\usepackage{bbding}
\usepackage[misc]{ifsym}
\usepackage{media9}
\usepackage{hyperref}
\usepackage{hyperref}\hypersetup{
colorlinks = true, citecolor=blue, linkcolor=blue,}
\usepackage{media9}   
\usepackage{graphicx,dblfloatfix}
\usepackage{xr}
\usepackage[toc]{appendix}
%\externaldocument{appendix}

\usepackage{geometry}
 \geometry{
 a4paper,
 left=35.25mm,
 right=35.25mm,
 top=30mm,
 }


\journal{bioRxiv}


\begin{document}

\begin{frontmatter}


\title{Efficient Inference on a Network of Spiking Neurons using \\Deep Learning}

%\title{Inference on a Network of Spiking Neurons}


\rmfamily

\author[1]{Nina Baldy}
\author[2]{Marmaduke M. Woodman}
\author[3]{Martin Breyton}
\author[4]{Viktor K. Jirsa $^\dag$\corref{cor}}
\ead{viktor.jirsa@univ-amu.fr}
\author[5]{Meysam Hashemi\corref{cor} $^\dag$\def\thefootnote{\dag}\footnotetext{These authors contributed equally.}}
\ead{meysam.hashemi@univ-amu.fr}


\address[1]{Aix Marseille Univ, INSERM, INS, Inst Neurosci Syst, Marseille, France}


\cortext[cor]{Corresponding author}

\begin{abstract}

The process of making inference on networks of spiking neurons is crucial to decipher the underlying mechanisms of neural computation. Mean-field theory simplifies the interactions between neurons to produce macroscopic network behavior, facilitating the study of information processing and computation within the brain. In this study, we perform inference on a mean-field model of spiking neurons to gain insight into likely parameter values, uniqueness and degeneracies, and also to explore how well the statistical relationship between parameters is maintained by traversing across scales. We benchmark against state-of-the-art optimization and Bayesian estimation algorithms to identify their strengths and weaknesses in our analysis. We show that when confronted with dynamical noise or in the case of missing data in the presence of bistability, generating probability distributions using deep neural density estimators outperforms other algorithms, such as adaptive Monte Carlo sampling. However, this class of deep generative models may result in an overestimation of uncertainty and correlation between parameters. Nevertheless, this issue can be improved by incorporating time-delay embedding. Moreover, we show that training deep Neural ODEs on spiking neurons enables the inference of system dynamics from microscopic states. In summary, this work demonstrates the enhanced accuracy and efficiency of inference on networks of spiking neurons when deep learning is harnessed to solve inverse problems in neural computation.
\end{abstract}




\begin{keyword}
Mean-field models \sep Bayesian inference  \sep Deep neural networks  \sep Simulation-based-inference \sep Hamiltonian Monte Carlo  \sep Optimization  \sep Bistability
\end{keyword}

\end{frontmatter}

\small
\textbf{Abbreviations}:

 MF, Mean-Field; QIF, Quadratic Integrate-and-Fire; SBI, Simulation-Based Inference;  NPE, Neural Posterior Estimation; MCMC, Markov chain Monte Carlo; HMC, Hamiltonian Monte Carlo; MAP, Maximum a Posteriori; DE, Differential Evolution; PSO, Particle Swarm Optimization; BO, Bayesian Optimization; ODEs, Ordinary Differential Equations; SDEs, Stochastic Differential Equations; Neural ODEs, Neural Ordinary Differential Equations\\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{Introduction}

Neural computation involves the complex information processing performed by spiking neurons, where their interactions within neural circuits contribute to higher-level computations and cognitive functions. Consequently, the cornerstone of theoretical neuroscience lies in the use of mathematical models to understand the intricacies of neural computation \citep{Dayan2005, Hertz2018}. This approach enables researchers to explore the complexities of neural processes, thereby revealing insights into the mechanisms that drive brain function and behavior. These mathematical models can range from simple idealized representations of single neurons \citep{Hopfield1982, Izhikevich2003} to complex network models that simulate the interactions within neural circuits \citep{Marder1998, Sussillo2014, Leary2015,  Bittner2021}. 
The biological neural computation forms the basis for the brain's ability to execute various inference tasks, such as recognizing patterns, making decisions, and generating responses to stimuli. This capability relies on the collective behavior of spiking neurons, where the interactions and coordination among these neurons within neural circuits enable the brain to process, integrate, and interpret sensory information, ultimately leading to higher-level cognitive functions and adaptive behavior \citep{Kandel2000, Gerstner2014, Friston2009, Friston2017}. 


Mean-Field (MF) models serve as effective computational abstractions that represent the collective behavior of large populations of neurons, while maintaining a degree of mathematical tractability \citep{Amari1977, Wilson1973, Jirsa1996, David2003, Deco2008, Hutt2015, Coombes2018, Bandyopadhyay2021, Cook2022}. Hence,  MF models facilitate the study of information processing and computation within the brain, which underlie cognitive processes such as perception, memory, and learning. Nevertheless, deriving and validating MF models from spiking neural networks present many challenges, particularly when assessing how the interaction between individual neurons and parameters leads to macroscopic behavior that aligns with the averaged activity of neural populations.


Recently, an analytically-driven MF model of spiking neurons has been formulated, which can effectively describe all potential macroscopic dynamical states of the network, including states of synchronous spiking activity \citep{Montbrio_Pazo_Roxin}. However, the operation of such complex systems (governed by coupled nonlinear differential equations) is determined by the selection of (biological or phenomenological) parameters, which when set in a specific configuration, give rise to a measurable signature of a computation \citep{Achard2006, Sussillo2014}. Analyzing MF models and comparing their emergent dynamics against a network of neurons involves solving inverse problems to ascertain the optimal parameter setting. This process requires swift outcomes to inform real-time decisions and also to deal with observation and dynamical noise. Yet, even in the simplest models, there can be a degenerate relationship between the model parameters and its overall emergent function \citep{Edelman2001, Prinz2004, Alonso2019}, making the inverse problem more challenging. 

Maintaining the inter-dependency between parameters by traversing across scales adds an additional layer of complexity to the validation process. It is crucial to distinguish between genuine, biologically relevant correlations, and artificial correlations that may arise from the inference process or modeling assumptions. Furthermore, due to the intricate nature of computation within neural circuits, it becomes intractable to analytically derive MF models that include more biological realism (such as adaption, neuromodulation, extra-synaptic transmission, and  E/I ratios). Statistical inference offers an efficient and adaptable approach to solving the inverse problem by identifying approximate parameter distributions that are responsible for generating computations in a biologically realistic model \citep{Achard2006, Liepe2014, Lueckmann2017, Goncalves2020, Bittner2021, Mlynarski2021}. 


The performance of statistical inference algorithms depends on the task, and there is no universally best algorithm for different inverse problems. Therefore, we conducted a benchmarking analysis against state-of-the-art optimization and Bayesian estimation algorithms to discern their respective advantages and limitations.
In practice, optimization methods are commonly used to quickly determine unknown quantities through a single point estimate \citep{Mendes1998, Nocedal1999, Kelley1999, Floudas2009}. These methods involve iteratively adjusting parameters to minimize or maximize an objective function, scoring the model's performance against observed data (e.g. through minimizing distance errors or maximizing correlation; \cite{Banga2008, Tashkova2011, Svensson2012, Hashemi2018}). 


The Bayesian approach offers a principled method for making inferences, predictions, establishing relationships between parameters, and quantifying uncertainty in the decision-making process \citep{BDA, Bishop, Gelman2020, VanSchoot2021}. In Bayesian modeling, all model parameters are treated as random variables and their values are subject to variation based on their underlying probability distributions. Such probabilistic techniques provide the full posterior distribution of unknown quantities hidden in the underlying data generating process. The uncertainty and inter-dependency in Bayesian estimation are naturally quantified by assigning a probability distribution to the parameters (known as the prior distribution), which is then updated based on the information provided by the data (referred to as the likelihood function). To conduct a full Bayesian procedure, the state-of-the-art MCMC method is adaptive Hamiltonian Monte Carlo (HMC; \cite{Duane1987, Neal2010, Hoffman2014}), which utilizes gradient information to avoid random walk behavior, enabling efficient sampling from high-dimensional distributions that may exhibit strong correlations \citep{Betancourt2017}.



Simulation-Based Inference (SBI; \cite{Cranmer2020, Brehmer2021}) or likelihood-free inference \citep{Papamakarios2016,  Brehmer2020} leverages deep generative models to conduct approximate Bayesian estimation, using low-dimensional data features that are generated by random simulations \citep{Goncalves2020, Lueckmann2021, Boelts2022}. In this efficient approach, a simple base probability distribution (prior) is transformed into a more complex distribution (posterior), through a sequence of invertible transformations (i.e., Normalizing Flows; \cite{Rezende2015, Papamakarios2019}). Notably, it allows for direct estimation of joint posterior distributions, bypassing the need for MCMC sampling \citep{Greenberg2019, Papamakarios2019b}. Moreover, expressive deep generative models have the potential to capture parameter nonlinear relationships between parameters and multi-modalities in the distributions \citep{Hashemi2023}.


Data-driven methods for learning dynamical models from time-series data have been extensively researched for several decades \citep{Juang1994, Ljung1998, Brunton2016, Linderman2017, Duncker2019, Koppe2019, Sip2023}.
Instead of relying on discretized maps, Neural Ordinary Differential Equations (Neural ODEs; \cite{Chen2018}) form a new family of deep neural network models for modeling continuous-time dynamics. Neural ODEs define the vector fields and ODE solution as a black-box differential equation solver, allowing for uncovering the dynamics of a system even when the governing equations are unknown \citep{Dupont2019, Bilovs2021}. This data-driven approach involves parameterizing system dynamics as continuous functions, enabling smooth and uninterrupted modeling of temporal evolution \citep{Yan2019, Kim2021}. Neural ODEs naturally adapt to varying time intervals and can accommodate fluctuations in the frequency of data observations \citep{Zhu2022, Goyal2023}.


Through an exploration of the aforementioned methods, we demonstrate that global optimization algorithms, such as Differential Evolution algorithm, offer fast and accurate point estimation of the true generative parameters when the dynamical noise is absent. However, when dealing with dynamic evolution that are subject to noise, SBI using deep neural density estimators emerges as the superior approach, outperforming other algorithms, such as adaptive HMC sampling. Additionally, when dealing with missing data (such as population firing rate) in state-space modeling, HMC fails to capture the dynamics of bistable switching behaviour. Instead, SBI is able to accurately recover the diverse dynamics in the phase-space representation. Nevertheless, this approach may lead to an overestimation of uncertainty and correlation between parameters, which can be mitigated by using time-delay embedding technique to improve the results. We validate this by employing a MF model of quadratic integrate-and-fire (QIF) neurons, which demonstrates that the inter-dependencies between parameters are maintained when traversing across scales. Finally, we demonstrate that training deep Neural ODEs on spiking neurons enables the inference of vector fields at macroscopic level. This allows for the prediction of emergent behaviors and system dynamics based on the microscopic state of the spiking neurons. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Materials and methods}
\label{Methods}

\subsection{A mean-field model of spiking neurons}

The quadratic integrate-and-fire (QIF) neurons are a class of simplified computational models that are extensively used to study the dynamics of spiking neurons \citep{Gerstner2002, Izhikevich2007}. In the QIF model, the membrane potential of neuron evolves according to a quadratic differential equation until it reaches a threshold, at which point the neuron emits a spike and the potential is reset.

\cite{Montbrio_Pazo_Roxin} have proposed a MF model that accurately describes macroscopic states of populations of firing neurons. This mechanistic model derives the firing rate equations for networks of heterogeneous, all-to-all coupled QIF neurons, which is exact in the thermodynamic limit, i.e. for large numbers of neurons. Specifically, when considering specific distributions of heterogeneity, the Lorentzian ansatz yields a nonlinear system of two ordinary differential equations for the firing rate $r$ and mean membrane potential $v$ of the neuronal population:
\begin{subequations}
\begin{align}
    \dot{r} &= 2 r v +\Delta/\pi  \label{eq:mpr_r} \\
    \dot{v} &= v^2 - \pi^2 r^2 + J r + \eta+ I(t) \label{eq:mpr_v}
\end{align}
\label{eq:mpr}
\end{subequations}
where $\eta$ is the average excitability, $J$ denotes the synaptic weight, and $\Delta$ indicates the spread of the neuronal excitability distribution in the neural population. Depending on the parameter settings, the phase diagram exhibits three qualitatively distinct regions: a single stable node, which represents a low-activity state; a single stable focus (spiral), which generally corresponds to a high-activity state; and a region of bistability, where both low and high firing rates can coexist (see \autoref{fig:MPR_Istep_phaseplanes}). This model has succeeded in establishing an exact correspondence between the time evolution of firing rate of the network and the underlying microscopic state of the spiking neurons \citep{Montbrio_Pazo_Roxin}.

  


\subsection{Inference methods}

We validate the mean-field approximation by comparing it against detailed neurons, using various parameter estimation inference methods. To evaluate these methods, we use synthetic data generated from the MF model and a network of QIF neurons.
Inference methods investigated in this work can be broadly divided into two classes:

(i) Optimization methods that return a point estimate of best fit based on the minimizing of a cost function, such as chi-squared error criterion defined by 
\begin{equation}
\label{eq:khi}
{\chi^2}({\theta})= \sum\limits_{i=1}^{N_t} \left({\hat x}(t_i, \theta)-{x}(t_i)\right)^2,
\end{equation}
where $x(t_{i})$ denotes the observed data at time points $t_i$ with $ i \in \{1, 2, \dots, N_t\}$, and ${\hat x}(t_i,{ \theta})$ represents the corresponding model prediction. Here $\theta \in \{ \eta, J, \Delta \}$ is the set of unknown parameters, and the set of observation combines activity of both $r$, $v$ (unless it is missing). Assuming no prior information and a Gaussian likelihood function with uncorrelated noise, this casts as a Maximum Likelihood Estimation (MLE) problem \citep{Hashemi2018}.

(ii) Bayesian methods return a posterior distribution of parameters, $ p(\theta \mid x)$, which represents an ensemble of parameter sets that are plausible given the observed data. Given the data $x$ and model parameters $\theta$, Bayes rule defines the posterior distribution as
\begin{equation}
    p(\theta \mid x) = \frac{p(\theta) p(x \mid \theta)}{p(x)}. \label{eq:Bayes_rule}
\end{equation}
The prior information $p(\theta)$ is typically determined before seeing the data (through beliefs and previous evidence). The likelihood function $p(x \mid \theta)$ represents the probability of some observed outcomes given a certain set of parameters (the information provided by the observed data). The denominator $p(x)=\int p(x \mid \theta)p(\theta)d\theta$ represents the model evidence or marginal likelihood, which amounts to simply a normalization factor.


From the optimization methods, we use the global search algorithms that incorporate a bio-inspired random search principle: Differential Evolution (DE; \cite{Storn1997, Price1999}), and Particle Swarm Optimization (PSO; \cite{Kennedy1995, Eberhart1995}). These algorithms do not require an initial guess for the parameters or the gradient information of the objective function \citep{Hashemi2018}.  We also consider Bayesian Optimization (BO; \cite{Snoek2012, Shahriari2015}), an algorithm that constructs a probabilistic model for the objective function using Gaussian processes. This approach allows for the integration of uncertainty in the optimization process.


From the Bayesian methods, we compare the results of two state-of-the-art Bayesian computation algorithms: Hamiltonian Monte Carlo (HMC; \cite{Duane1987, Neal2010}) which is unbiased and exact in infinite runs, and a Simulation-Based Inference (SBI; \cite{Cranmer2020, Brehmer2021}) which approximates the posterior using deep generative models. Here, generative modeling is an unsupervised machine learning method to model a probability distribution based on the samples drawn from that distribution.  From the Bayesian methods, we also report the results of Maximum a Posteriori (MAP) estimation. 



\subsection{Hamiltonian Monte Carlo}

Markov chain Monte Carlo (MCMC) is a powerful class of computational algorithms used for sampling from a distribution, in which the sampling process does not require knowledge of the entire distribution, making it a versatile tool. \citep{Andrieu2003, Murphy2022, Mcelreath2020}. MCMC is unbiased and asymptotically exact, in the limit of infinite runs. Hamiltonian Monte Carlo (HMC; \cite{Duane1987, Neal2010}) is a gradient-based MCMC designed to avoid random walk behaviour, and it can efficiently sample from high-dimensional distributions that may exhibit strong correlations \citep{Betancourt2017}. However, the efficiency of HMC is sensitive to the algorithm parameters. 

In this study we use a self-tuning variant of HMC (known as the No-U-Turn Sampler; \cite{Hoffman2014}) from a high-level statistical modeling tool called Stan \citep{Carpenter2017}.
In particular, the NUTS calibrates the number of steps and step-size of the leapfrog integrator (in solving the Hamiltonian equations of motion) during a warm-up phase to achieve a target Metropolis acceptance rate. For more details see \cite{Betancourt2013, Baldy2023}. Moreover, Stan offers alternative methods such as MAP estimation using L-BFGS optimization, automatic differentiation for efficient gradient computation, and various diagnostics to assess the convergence of the inference process (see \url{https://mc-stan.org}). 



\subsection{Simulation-Based Inference}

Simulation-Based Inference (SBI) conducts efficient Bayesian inference for complex models when the calculation of the likelihood function is either analytically or computationally intractable \citep{Cranmer2020, Brehmer2021}.
In computational models, where the data can be generated through stochastic simulations, SBI leverages repeated simulations from the generative model and employs probabilistic machine learning to estimate a target probability distribution. Instead of directly sampling from distributions using MCMC or explicitly evaluating the likelihood function, SBI overcomes these challenges by using deep neural density estimators, such as Masked Autoregressive Flows (MAF; \citep{Papamakarios2017}).  These density estimators learn an invertible transformation between the distributions of parameters and low-dimensional data features at a very low computational cost, to efficiently sample from distributions. 

Taking the prior distribution $p(\theta)$ over the parameters $\theta$, a limited number of $N$ simulations are generated for training step from the generative model i.e.,  $\{(\theta_i, x_i)\}_{i=1}^{N} \sim p(\theta, x)$. After the training step, we are able to quickly estimate the approximated posterior $q_{\phi}(\theta \mid x)$ with learnable parameters $\phi$, so that for the observed data $x_{obs}$: $q_{\phi}(\theta \mid x_{obs}) \simeq p(\theta \mid x_{obs})$. For more details see \cite{Goncalves2020, Hashemi2023}. 

The methods for SBI often include a sequential training procedure, which adaptively guides simulations to yield more informative estimates \citep{Papamakarios2019b, Lueckmann2019, Durkan2020, Wiqvist2021, Deistler2022}. In particular, Sequential Neural Posterior Estimation (SNPE; \cite{Greenberg2019, Goncalves2020})  dynamically refines the proposals, network weights, and posterior estimates to learn the relationship between model parameters and the observed summary statistics of the data. In this study, we used SNPE with a single round to take advantage of an amortized strategy; After incurring an initial computational cost for the simulation and training steps to learn all the joint posterior distributions, then the posterior can be quickly estimated from any new observations (by a forward pass through neural networks)  without any additional computational overhead or further simulations.




\subsection{Time-delay embedding}

Time-delay embedding is a commonly used technique for characterizing dynamical systems based on limited measurements, time-series analysis, and prediction \citep{Takens2006}. In time-delay embedding, the reconstruction of a latent high-dimensional system relies on incorporating incomplete measurements along with a temporal history of preceding measurements to create a comprehensive representation \citep{Kennel1992, Hirsh2021}.
In a subsequent analysis, we challenge the inference process by assuming that the firing rate activity $r$ is not directly observed (missing data problem). Instead, to improve the inference, we recovered the latent time-series $r_{rec}$ from the observed activity $v$, which is coupled to $r$ according to Eq.~\ref{eq:mpr}.
By expanding on a method introduced by \cite{Abarbanel}, which primarily focuses on predicting physical variables in time-delay embedding, we removed the assumption that we have the access to training data points from the hidden time-series. Instead, we leverage our understanding of the generator to simulate data pairs $(r, v)$ that can be used for training purposes.

Time-delay embedding requires the setting of hyperparameters, such as the delay (time lag), $T$, and  the dimension of the embedding space, $d$. These hyperparameters are typically set prior to training, often based on the minimization of mutual information to determine the appropriate delay and the false nearest neighbors method to determine the number of embedding dimensions \citep{Kennel1992, Tan2023}. However, in the present application, we have found that the hyperparameters suggested by these methods were not the most effective in achieving an accurate fit. Instead, we select hyperparameters that minimize the mean square error of the fit to simulated data, with $T=160$ points (i.e., $0.16~sec$) and $d=12$. To do this, a set of 100 pairs of coupled time-series $(r, v)$ was simulated, with an observation noise intensity of 0.1 and varying parameters ($\Delta, \eta, J$). These pairs were then used to infer regression coefficients that closely match $r$ to the delay embedding space representation of $v$.




\subsection{Neural ODEs}

% 2-3 lines of concept+2-3 lines on the details of implementation: layers, loss function etc
Neural ODEs are a set of machine learning techniques that allow to reconstruct the phase-space of a dynamical system from a training set of observations \citep{Chen2018}. In a first analysis, we trained a Neural ODE on time-series from the MF model, and then applied the same method to the data generated by a network of QIF spiking neurons. If $\textbf{x}(t)$ is the vector of state variables governed by the dynamical system $\dot{\textbf{x}} = f(\textbf{x}, \theta,I_{ext})$, one can use a Neural ODE to approximate the function $f$ with an artificial neural network $F_{\phi}$, yielding the corresponding dynamical equation $\dot{\hat{\textbf{x}}} = F_{\phi}(\hat{\textbf{x}},\theta,I_{ext})$. $F_{\phi}$ is learned through backpropagation, minimizing the loss function for each training example of length $T$: 
\[L_{\phi}(\hat{x},x)={\sum_{t=0}^{T}(\hat{x}(t, \theta)-x(t))^2}\]
where $\hat{x}$ and $x$ are times-series generated by $F_{\phi}$ and $f$,  respectively, with the same integration scheme (e.g., Heun's method). In this study, a Neural ODE was implemented in JAX \citep{jax2018github} by constructing a multi-layer-perceptron (MLP) with one hidden layer of 16 units, and with hyperbolic tangeant activation functions. The cost function was the average loss functions across training examples $\mathcal{L_{\phi}}=\frac{1}{N*T}\sum_{n=1}^{N}L_{\phi,n}$. 
At each training iteration, the initial $(r, v)$ values of each segment is fed to the Neural ODEs and the forecast trajectory according to current state of the $F_{\phi}$ enters the cost function, then minimized through backpropagation using the Adam optimizer. 




\subsection{Software and algorithmic setup} 

We use Python implementations of the optimization and Bayesian algorithms: DE from SciPy \citep{Virtanen2020scipy}, PSO from the toolkit PySwarms \citep{Miranda2018pyswarms}, and BO from the package Bayesian Optimization \citep{Nogueira2014BO}. DE is set with population size of 10, the maximum iterations of 500. PSO is set with 10 particles, and 500 iterations. BO is set with 50 random initialisation, 500 iterations, and kappa=100 (higher values increase exploration).

The MAP estimation is obtained using a quasi-Newton L-BFGS optimizer from Python interface to Stan \citep{Carpenter2017}, an open-source probabilistic programming language for Bayesian modeling and statistical inference. We also use Stan's implementation of the NUTS for fully Bayesian estimation by HMC, with target Metropolis acceptance rate of 0.8, maximum tree depth of 10, warm-up iterations of 1000, and 500 sampling phase iterations. 

Finally, for SBI we used Sequential Neural Posterior Estimation (SNPE; \cite{Greenberg2019, Goncalves2020}) from the PyTorch-based SBI toolkit \citep{Tejero2020sbi}. SNPE was run for a single round with 100,000 simulations, unless specified otherwise. For training step, we used a MAF, with 5 autoregressive layers, each with two hidden layers of 50 units.
The set of data features includes the statistical moments of time-series up to the fourth order (mean, standard deviation, skewness, and kurtosis), and the peak properties (number and location of first peak, as shown in \autoref{fig:DataFeatures}).


Parameter bounds for optimization, and  uniform prior for Bayesian inference, were set as: $\Delta \in [0.1, 5]$, $\eta \in [-10, -3]$, $J \in [5, 20]$. For the simulation of Eq.~\ref{eq:mpr}, we used Euler-Maruyama integration for $t=100~sec$ with a time step of $dt=1~msec$. The ground truth parameters were set as $\eta=-4.6$, $J=14.5$, and $\Delta=0.7$, ensuring that the system is in a bistable regime, unless specified otherwise. Moreover, a step current with amplitude of $3~v$ from time $30~sec$ to $60~sec$ was added to the $v$ variable. Each simulation took around $0.01~sec$ to run using a Just-In-Time (JIT) compiler.
To generate stochastic dynamics, a zero-mean Gaussian noise with $\sigma=0.1$ was added to the both $r$, and $v$ variables in Eq.~\ref{eq:mpr}.  The prior on dynamical noise was set as $\sigma \in [0, 1]$.
For comparison with the MF model, we ran a network of $10^4$ all-to-all connected QIF neurons using the Brian simulator \citep{Stimberg2019}.
 
The model simulation and parameter estimation were performed on a Linux machine with 3.60 GHz Intel Core i7-7700 and 8 GB of memory.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Results}
\label{Results}

We present the results of the optimization and Bayesian algorithms for three cases: (i) inferring from exact deterministic synthetic data, (ii) inferring from stochastic synthetic data (driven by dynamical noise), and (iii) inferring with stochastic synthetic data when the activity for $r$ is unknown (missing data).  We report and compare the goodness-of-fit using the root mean square error (RMSE) to the true (observed or hidden) time-series and parameters, the variance in the estimation process, and the computational cost.



Before running inference process, we conduct a sensitivity analysis to assess the structural identifiability of the parameters. This analysis involves calculating local sensitivity coefficients, which measure the effect of small adjustments in a parameter on the model's output while keeping all other parameters constant. A small change in a sensitive parameter leads to a significant alteration in the model output, indicating its identifiability. Conversely, when there are no alterations in the model output despite adjustments in a parameter, it suggests the parameter's non-identifiability. To assess the identifiability of the parameters, we used the profile likelihood \citep{Raue2009, Wieland2021} and Hessian matrix, a metric describing the local curvature of a function based on its second partial derivatives \citep{Hashemi2018, Hashemi2023}. Our results indicate that all three parameters $\Delta$, $\eta$, and $J$ can be identifiable, however, the profile likelihood obtained using Eq.~\ref{eq:khi} reveals the presence of numerous local minima (see \autoref{fig:SensitivityAnalysis}). This highlights the difficulty in attaining the global minimum during the inference process.



\subsection{Inference on deterministic synthetic data}

\begin{figure}[p]
    \centering
    \includegraphics[width=\linewidth]{Figs/Fig1.png}
    \caption{Inference on deterministic data.  
     (\textbf{A}) The trajectories in phase-plane exhibiting bistable dynamics, and the corresponding time-series of firing rate ($r$) and membrane potential ($v$), used as the observations for inference (ground truth: $\Delta=0.7$, $\eta=-4.6$, $J=14.5$). (\textbf{B}) The estimated trajectories in phase-plane, along with the corresponding parameters displayed in the top panels. All the algorithms capture the bistable dynamics in a qualitative manner. (\textbf{C}) For parameters $\Delta$, $\eta$, and $J$, the point estimations are displayed in the top panels along with the profile likelihood (in grey), while the full posterior distributions are shown in the bottom panels. HMC leads to more precise parameter estimates with lower uncertainty compared to SBI.  (\textbf{D}) Accuracy in estimation based on the sum over RMSE values. The bootstrap uncertainty is calculated for time-series (top panel) and parameters (bottom panel) through multiple runs. (\textbf{E}) Computational cost for each inference algorithm.  Overall, DE excels in both speed and accuracy, but it offers only a point estimate. HMC is exact and provides informative posterior estimates, but it is very computationally prohibitive. Rather, SBI effectively provides accurate estimates along with associated uncertainties at a reasonable computational cost. DE, Differential Evolution; PSO, Particle Swarm Optimization; BO: Bayesian Optimization; MAP, Maximum a Posteriori; HMC: Hamiltonian Monte Carlo;  SBI: Simulation-Based Inference.
    } 
    \label{fig:ODEdata}
\end{figure}



Here, we compare the inference results of different algorithms when we have complete observations of both state variables $(r,v)$, and the system operates without any noise (see \autoref{fig:ODEdata}). The observed time-series and trajectories in phase-plane, which exhibit a bistability between a stable node and a stable focus, are illustrated in \autoref{fig:ODEdata}\textbf{A}. From the results demonstrated in \autoref{fig:ODEdata}\textbf{B}, it can be seen that all the algorithms qualitatively reproduce the bistability behaviour in the phase-plane. However, when evaluating their performance by estimating the true generative parameters, all optimization methods, except DE, get stuck in a local minima (\autoref{fig:ODEdata}\textbf{C}). The results indicate that DE, SBI and HMC algorithms correctly recover the ground-truth parameters, the former as an almost exact point estimate (\autoref{fig:ODEdata}\textbf{C}, top panels), while the latter two, SBI and HMC, yield full posterior distributions (\autoref{fig:ODEdata}\textbf{C}, bottom panels).  In terms of uncertainty quantification, HMC offers a more informative posterior distribution compared to SBI (see \autoref{fig:ODE_joint_posterior}).

When it comes to matching the observed time-series, the optimization algorithms such as PSO and BO deviate significantly from true values, while other approaches equally retrieve an almost perfect fit to the observed time-series (see \autoref{fig:ODEdata}\textbf{D}, top panel). Nevertheless, MAP largely fails to accurately estimate model parameters due to overfitting (\autoref{fig:ODEdata}\textbf{D}, bottom panel). This type of overfitting emphasizes the importance of quantifying uncertainty to verify the reliability of inference, going beyond a point estimation. Note that for HMC and SBI, we report the results using posterior predictive check i.e., re-generating data using random parameters drawn from the estimated posterior and then comparing simulations with the observed data.

In terms of computational cost for inference, DE has a clear advantage with its rapid performance, typically taking less than a minute to complete (\autoref{fig:ODEdata}\textbf{E}). Despite a high precision, the computational cost of HMC in this example is prohibitively expensive, taking almost a hundred hours to complete the inference process. On the other hand, SBI was terminated in nearly one and a half hours (including 100k random simulations, training, and sampling), making it approximately 60 orders of magnitude faster than HMC. Additionally, due to the amortized approach adopted by SBI, each sampling process takes less than one minute to estimate the joint posterior distributions from new data. 

In summary, DE demonstrates superior speed and accuracy, but it only provides a point estimate. Among Bayesian methods, HMC is exact and provides certain estimates, but it is computationally prohibitive. On the other hand, SBI effectively provides accurate estimates along with associated uncertainties at a reasonable computational cost (see~\autoref{tab:ODEdata},  \href{run:https://github.com/ins-amu/Inference_MFM/blob/main/Videos/Movie1_MPR_ODE_HMC_RV.mp4}{Movie 1}, and  \href{run:https://github.com/ins-amu/Inference_MFM/blob/main/Videos/Movie2_MPR_ODE_SBI_RV.mp4}{Movie 2}). 



\subsection{Inference on stochastic synthetic data}

The inherent randomness in stochastic systems introduces uncertainty into the behavior of the system under study, which makes it challenging to accurately identify its underlying dynamics. In particular, the presence of dynamical noise significantly increases the complexity of the inference process and necessitates the use of robust probabilistic methodologies that can effectively account for and handle the stochastic nature of the data. 


\begin{figure}[p]
    \centering
    \includegraphics[width=\linewidth]{Figs/Fig2.png}
    \caption{Inference on noisy data.  (\textbf{A}) The generated bistable dynamics in the phase-plane and the corresponding time-series of firing rate ($r$) and membrane potential ($v$) used as the observations for inference (ground truth: $\Delta=0.7$, $\eta=-4.6$, $J=14.5$).  (\textbf{B}) The estimated trajectories in the phase-planes, along with the corresponding parameters displayed in the top panels. All the algorithms qualitatively capture the bistable behavior. (\textbf{C}) The point estimation along with the profile likelihood (top panels) and the full posterior (bottom panels) for parameters $\Delta$, $\eta$, and $J$. SBI leads to more precise parameter estimates with lower uncertainty compared to HMC. (\textbf{D}) Accuracy in estimation based on the sum over RMSE values for the time-series (top panel) and parameters (bottom panel). The bootstrap uncertainty is calculated through multiple runs. (\textbf{E}) Computational cost for each inference algorithm.  When evaluating overall accuracy, uncertainty quantification, and computational cost, then SBI outperforms all other algorithms.
    }
    \label{fig:SDEdata}
\end{figure}


Here, we report the results of inference on synthetic data with zero-centered Gaussian dynamical noise, where the intensity is $\sigma = 0.1$  (see \autoref{fig:SDEdata}).
The observed noisy time-series and trajectories in phase-plane, exhibiting a bistable behavior between a stable node and a stable focus, are illustrated in \autoref{fig:SDEdata}\textbf{A}. From the results demonstrated in \autoref{fig:SDEdata}\textbf{B}, we can see that all the algorithms qualitatively replicate the bistability behavior in the phase-plane. Yet, when assessing how well they recover the true generative parameters, only DE yields an accurate point estimate among the optimization algorithms (\autoref{fig:SDEdata}\textbf{C}, top panels). 

In terms of uncertainty quantification, SBI generates posteriors that are tightly centered on the ground-truth parameters, in comparison to the HMC sampling (see \autoref{fig:SDEdata}\textbf{C}, bottom panels, and \autoref{fig:SDE_joint_posterior}). A detailed comparison of convergence diagnostics indicates that both SBI and HMC methods provide ideal Bayesian estimation (see \autoref{fig:Zscores_Shrinkage_SDE}).
Regarding the proximity to the observed time-series and true parameters, both HMC and SBI offer a closer match compared to the other algorithms (\autoref{fig:SDEdata}\textbf{D}). This highlights the challenges inherent in inferring stochastic systems via optimization algorithms, as the error metrics such as RMSE used to define the objective function may not reliably gauge accuracy. 

In terms of computational efficiency for inference, the running time of all algorithms increases when the noise is present, except for HMC (\autoref{fig:SDEdata}\textbf{E}). Nevertheless, when considering the entire process (including random simulations, training, and sampling), SBI remains approximately 8 orders of magnitude faster than HMC. Interestingly, SBI is also able to accurately and efficiently estimate the dynamical noise in the system (see ~\autoref{fig:SBI_SDE_Istep_RV_NoiseEstimation}). 

In summary, these results demonstrate that when considering overall accuracy in the presence of noise and bistability, uncertainty quantification, and computational cost, the SBI outperforms all other algorithms, including HMC (see~\autoref{tab:SDEdata},  \href{run:https://github.com/ins-amu/Inference_MFM/blob/main/Videos/Movie3_MPR_SDE_HMC_RV.mp4}{Movie 3}, and  \href{run:https://github.com/ins-amu/Inference_MFM/blob/main/Videos/Movie4_MPR_SDE_SBI_RV.mp4}{Movie 4}). 


\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{Figs/Fig3.png}
    \caption{The performance of inference algorithms with increasing the intensity of dynamical noise using the sum over: (\textbf{A}) RMSE of the time-series, and (\textbf{B}) RMSE of the model parameters. Overall, with a sufficient number of simulations for training, the SBI emerges as the most accurate and robust method in our algorithmic benchmark.
    }
    \label{fig:NoiseSweep}
\end{figure}


Next, we investigate how varying the intensity of dynamical noise impacts the inference on stochastic data. Our results indicate that the overall quality of fit to the noisy data (\autoref{fig:NoiseSweep}\textbf{A}) and the accuracy of recovered parameters (\autoref{fig:NoiseSweep}\textbf{B}) remain stable up to $\sigma \leq 10^{-1}$ for all algorithms. However, both metrics exhibit a significant increase beyond this threshold, particularly for optimization methods. 

The MAP estimation is robust with regard to the amount of dynamical noise, but it tends to overfit. This is because the MAP estimation consistently provides the least accurate inference on parameters, even though its fit to the time-series data is almost perfect. In contrast, Bayesian inference methods such as HMC and SBI are overall more accurate and significantly more robust compared to optimization methods.  Both HMC and SBI exhibit significantly lower error than others, even at high noise levels (e.g., $\sigma = 1$). 

Interestingly, SBI appears to be more resilient to high levels of noise compared to HMC. Given a sufficient number of simulations for training (e.g., 100k), SBI demonstrates the most accurate and robust fit to the data, consistently staying close to a perfect fit across various noise values. See \autoref{fig:SBI_ZandShrink} for a systematic investigation on the impact of the number of simulations on the performance of SBI.  Overall, these results indicate that SBI is the most accurate and robust algorithm in our benchmark.


\subsection{Inference on missing data in the state-space}

Here, we explore the performance of inference methods in situations where data is available for only one of the variables in the state-space modeling. We consider the mean membrane potential $v$ as the observed data, which is simulated with dynamic noise of intensity $\sigma = 0.1$, while the firing rate $r$ remains latent (\autoref{fig:Missingdata}\textbf{A}). The noise intensity is fixed for inference process.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figs/Fig4.png}
    \caption{Inference on noisy observation with missing data. The generated bistable dynamics in the phase-plane and the corresponding time-series of observed membrane potential ($v$) and hidden firing rate ($r$), with ground truth: $\Delta=0.7$, $\eta=-4.6$, $J=14.5$.  (\textbf{B}) The estimated trajectories in the phase-planes, along with the corresponding parameters displayed in the top panels. Only DE and SBI provide a close agreement with the observed bistable trajectories. (\textbf{C}) The point estimation along with the profile likelihood (top panels), and the full posterior (bottom panels) for parameters $\Delta$, $\eta$, and $J$. (\textbf{D}) The accuracy of estimation is evaluated by summing the RMSE values for both the true time-series (top panel) and parameters (bottom panel),  while considering the bootstrap uncertainty through multiple iterations. (\textbf{E}) The computational cost for each inference algorithm.  Overall,  DE and SBI outperform other algorithms in inferring the bistable dynamics when dealing with missing data.
    }
    \label{fig:Missingdata}
\end{figure}


From \autoref{fig:Missingdata}\textbf{B}, we observe that only DE and SBI were capable of retrieving the true dynamics in the phase-plane when $r$ was missing. As shown in \autoref{fig:Missingdata}\textbf{C} (top panel), among optimization algorithms, only DE correctly estimates the true parameters. Notably, HMC fails considerably in this problem by proposing over-confident posterior distributions that are far from the ground truth (\autoref{fig:Missingdata}\textbf{C}, bottom panel). In contrast, the estimated posteriors using SBI are centered on the ground-truth parameters, but they exhibit a more diffuse uncertainty compared to the previous results (see \autoref{fig:SDE_missingdata_joint_posterior}).
Note that HMC fails to accurately reconstruct the latent variable $r$ from observed $v$ (see \autoref{fig:SDEmissingdata_timeseries}), even though its error on the fitted trajectories is comparable to that of SBI (\autoref{fig:Missingdata}\textbf{D}, top panel). Nevertheless, the unreliability in the results produced by HMC becomes evident when observing the RMSE values for model parameters (\autoref{fig:Missingdata}\textbf{D}, bottom panel). 

In terms of computational cost for inference (\autoref{fig:ODEdata}\textbf{E}), optimization by DE still has a clear advantage with its rapid performance.
Interestingly, SBI remains efficient, approximately 68 orders of magnitude faster than HMC (see~\autoref{tab:Missingdata},  \href{run:https://github.com/ins-amu/Inference_MFM/blob/main/Videos/Movie5_MPR_SDE_HMC_V.mp4}{Movie 5}, and  \href{run:https://github.com/ins-amu/Inference_MFM/blob/main/Videos/Movie6_MPR_SDE_SBI_V.mp4}{Movie 6}). Overall, SBI outperforms HMC in the recovery of the bistable dynamics, including the hidden firing rate.  However, this approach can lead to an overestimation in the associated uncertainty, when compared to the full observed state-space dynamics (see \autoref{fig:SDEdata} versus \autoref{fig:Missingdata}).



\subsection{Phase-space reconstruction}

As demonstrated in the previous section, the inference process posed a challenge when only the membrane potential $v$ was observed and the firing rate $r$ was missing. To improve the inference in such cases, we approximately reconstruct the hidden $r$ from the observed counterpart $v$, using the time-delay embedding technique (\autoref{fig:ReconstructedPhasePlane_and_corr}). 

As it can be seen from \autoref{fig:ReconstructedPhasePlane_and_corr}\textbf{A}, the reconstructed firing rates closely follow the original time-series (RMSE=0.151). This leads to accurately capturing the bistable switching behavior in the phase-plane, as shown in \autoref{fig:ReconstructedPhasePlane_and_corr}\textbf{B}. We subsequently explored how the access to the trajectories of state variables influences the statistical relationship between parameters. When both the firing rate ($r$) and membrane potential ($v$) are observed, both HMC and SBI algorithms reveal a strong correlation between the average excitability $\eta$, and the synaptic weight $J$ ($|\rho_{\eta, J}|= 0.78$), while the other parameters exhibit no codependency (see \autoref{fig:ReconstructedPhasePlane_and_corr}\textbf{C}). 

When the firing rate is latent and only the membrane potential is observed, the correlation between parameters is more pronounced, as estimated by HMC, while SBI tends to overestimate this co-dependency as fully degenerate (see \autoref{fig:ReconstructedPhasePlane_and_corr}\textbf{D}). This highlights the challenges in the inference or parameter estimation process when lacking access to complete knowledge of the system dynamics. This issue can be improved by using time-delay embedding to reconstruct the full phase-space dynamics and inform the inference process, as shown by the reduced correlation between parameters in  \autoref{fig:ReconstructedPhasePlane_and_corr}\textbf{E}. 


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figs/Fig5.png}
    \caption{Reconstruction of firing rate $r$ from mean membrane potential $v$, and comparison of inter-dependency between parameters. (\textbf{A}) Original and reconstructed mean firing rate. (\textbf{B}) Observed (left) and reconstructed (right) system dynamics in phase-plane. Correlations between estimated joint posterior distributions, using HMC (left) and SBI (right), when: (\textbf{C}) Both $r$ and $v$ are observed, (\textbf{D}) Only $v$ is observed and $r$ is hidden, (\textbf{E}) Reconstructed $r$ from the observed $v$ using time-delay embedding. }
    \label{fig:ReconstructedPhasePlane_and_corr}
\end{figure}



\subsection{Inter-dependency between generative parameters}

In the previous sections, we performed inference against the system dynamics derived from the MF model given by Eq. \eqref{eq:mpr}. We now aim to investigate how the inference of the posterior distribution and the relationships between parameters remains consistent by traversing across scales. This can be achieved by conducting inference using observed data generated by QIF neurons (at the microscopic level) versus the data generated by MF model (at the macroscopic level).

First, we compared the simulated membrane potential $v$ and firing rate $r$ using a MF model given by Eq.~\eqref{eq:mpr} to the averaged activities of a network of $10^4$ all-to-all connected QIF neurons (see raster plot shown in \autoref{fig:QIF_correlations}(\textbf{A})). \autoref{fig:QIF_correlations}(\textbf{B}) demonstrates that the MF model can accurately generate the (smoothed) transient dynamics that emerge from an ensemble of spiking neurons. However, the first spike emitted by the MF model after stimulation exhibits a short delay compared to the averaged QIF neurons. % why?
Then, we used the data generated by MF and QIF models as the observation, and conducted inference using the MF model through Bayesian estimation algorithms, such as HMC and SBI. Specifically, we compared the correlations between the estimated joint posteriors of model parameters in each case, as shown in \autoref{fig:QIF_correlations}(\textbf{C}). 

We observed consistent correlations between parameters across the two types of datasets (macroscopic MF and microscopic QIF) when fitted using the MF model. Our results indicate that the strong negative linear correlation between excitability and synaptic weight ($\rho_{\eta, J} \approx -0.78$) persists when using HMC and SBI against both datasets. Considering the agreement between HMC and SBI across two datasets, we can conclude that the consistent high correlation between parameters $\eta$ and $J$ is intrinsic and not induced by the inference process or model assumptions.



\begin{figure}
     \centering
     \includegraphics[width=\linewidth]{Figs/Fig6.png}
     \caption{Comparing the transient dynamics of a MF model with the emergent dynamics of a network of QIF neurons, and exploring the interdependency between parameters. (\textbf{A}) Raster plot of QIF neurons. (\textbf{B}) The membrane potential $v$ (top panel) and firing rate $r$ (bottom panel) generated by MF model (in cyan) versus averaged activities of QIF neurons, as the raw simulations (in light gray) and smoothed (dark grey). At time $t=30~sec$, a current $I_0=3~ mv$ is applied to all neurons, and set to zero again at $t=60~sec$. (\textbf{C}) The Pearson correlation coefficients between parameters in the MF model, estimated using Bayesian algorithms (HMC and SBI), against data generated by different models (MF and QIF). The strong negative linear correlation between excitability and synaptic weight ($\rho_{\eta, J} \approx -0.78$) persists when using both Bayesian algorithms against both datasets.}
     \label{fig:QIF_correlations}
 \end{figure}



 
\subsection{SBI on the stimulus current}

Here, we challenge SBI approach in inferring system dynamics while also accounting for an unknown input current, which plays a crucial role in emerging the bistable behavior. Given only the position or waveform of input currents, we estimate the intensity or angular velocity across various ground-truth values: $I_0$ in a step current as $I(t)=I_0 \cdot \mathbf{1}_{\text{stim}}(t)$ and $\omega_0$ in a sinusoidal current as $I(t)=\sin (\omega_0 t) \cdot \mathbf{1}_{\text{stim}}(t)$, as shown in \autoref{fig:SBI_stimulus}(\textbf{A}), and (\textbf{B}), respectively. 

Our results demonstrate that in both cases, the SBI approach accurately recovers the unknown parameters in the input currents. The posterior credibility intervals, visualized as error bars, indicate certain estimates that are close to a perfect fit ($y=x$, in red) across all different values. See \autoref{fig:SBI_Timeseries_ISin_IStep} for the observed and predicted time-series. This validates the capability of SBI in accurately estimating the system dynamics, even when the characteristics of the input current are unknown.


\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Figs/Fig7.png}
    \caption{SBI on the stimulus current. (\textbf{A}) Step current as $I(t)=I_0 \cdot \mathbf{1}_{\text{stim}}(t)$. (\textbf{B}) Sinusoidal current as $I(t)=\sin (\omega_0 t) \cdot \mathbf{1}_{\text{stim}}(t)$. Dashed red line represents a perfect fit.  }
    \label{fig:SBI_stimulus}
\end{figure}



\subsection{SBI on stability of system dynamics}

Here, we show that SBI can be used to investigate the stability of system dynamics from low-dimensional summary statics of observed time-series. \autoref{fig:SBI_PhaseDiagram} shows a phase diagram of the system as a function of the mean $\eta$ and synaptic weight $J$, both normalized by the width of the input distribution $\Delta$. Using linear stability analysis, there are three qualitatively distinct regions of the phase diagram: (i) A single stable node corresponding to a low-activity state (shown in blue), (ii) A single stable focus generally corresponding to a high-activity state (shown in red), and (iii) A region of bistability between low and high firing rate (shown in cyan).  

Interestingly, a similar basin of bi-stability in phase diagram can be readily reproduced using deep neural density estimators (such as MAF model) in SBI approach. By training on the low-dimensional data features extracted from the time-series (presence or absence of oscillations before, during, and after stimulation), the generated posterior samples display a very close agreement with the results obtained from linear stability analysis. This demonstrate the capability of SBI in accurately estimating system dynamics from summary statics, including the presence of bi-stability in the phase diagram.


 
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figs/Fig8.png}
    \caption{SBI over the stability of system dynamics in phase diagram. In the wedge-shaped region (shaded in cyan), bistability exists between a high and a low activity state. On the right side, a stable focus is indicated (shaded in red), while on the left side, a stable node is depicted (shown in blue). By training a deep neural density estimator on data features such as damped oscillations due to the input, the posterior samples generated using SBI accurately capture the bistable dynamics (shown in yellow), aligning closely with results from linear stability analysis. The black asterisk denotes the observation point used for inference. The insets display observed (dark blue) and predicted (light blue) time-series in different regimes: (\textbf{A}) bistability, (\textbf{B}) stable focus, and (\textbf{C}) stable node.}
    \label{fig:SBI_PhaseDiagram}
\end{figure}



\subsection{Neural ODEs on system dynamics}

In this section, our aim is to infer the collective dynamics of QIF neurons without making any assumptions about the underlying generating dynamics. To achieve this, we used Neural ODEs as a powerful tool for modeling continuous-time dynamics without assuming any prior knowledge of the underlying equations governing the system. Unlike traditional discrete-time models, Neural ODEs parameterize the continuous-depth formulation, allowing for seamless interpolation between observed data points.


We first validate Neural ODEs using the data garnered by MF model described by Eq.~\eqref{eq:mpr}.  We generated 3 different datasets using the MF model given by Eq.~\eqref{eq:mpr}, with a set of parameters corresponding to a bistable regime: $\{\Delta=1,J=15,\eta=-5\}$. For each datasets, we sampled the phase-space by varying initial conditions according to a regular grid so that $r_0\in[0.1, 3]$ and $v_0\in[-2,2]$. We then solved the system for 1000 time points (with an integration time step of $dt=0.01~sec$). To speed up training, each trajectory was downsampled by a factor of 10 and divided into 10 segments, each consisting of 10 time points. The training and test data were randomly split, with $75\%$ of the data used for training and $25\%$ used for testing. 


The first training set, comprising of $N=100$ deterministic trajectories, was generated (see \autoref{fig:NeuralODE_montbrio}\textbf{A}). The results indicate that the Neural ODE almost perfectly reconstructed the phase-space (see \autoref{fig:NeuralODE_montbrio}\textbf{B}). In the second training set, which had the same size, a dynamical noise with a standard deviation of $\sigma=0.1$ was added during the integration process (\autoref{fig:NeuralODE_montbrio}\textbf{C}). In this case, the estimated nullclines suffered from overfitting although without affecting the overall reconstructed dynamics, as the predicted trajectories were still very similar to the original data  (as shown \autoref{fig:NeuralODE_montbrio}\textbf{D}). Increasing the size of the training dataset (see \autoref{fig:NeuralODE_montbrio}\textbf{E}) significantly reduces overfitting.  As a result, the reconstructed nullclines show a very close agreement with those obtained from the deterministic data (\autoref{fig:NeuralODE_montbrio}\textbf{F}).
To illustrate overfitting, \autoref{fig:NeuralODEs_Fit_traces_MPR} shows the loss functions for the different scenarios, as well as snapshots of phase-space reconstruction during training. Overall, the Neural ODE successfully reconstructs the underlying deterministic system, even in the presence of noise (see \href{run:https://github.com/ins-amu/Inference_MFM/blob/main/Videos/Movie7_MPR_PhasePlane_NeuralODEs.mp4}{Movie 7}).


\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{Figs/Fig9.png}
    \caption{Example of training data generated from the MF model set in a bistable regime (top row). The trajectories are displayed after the 10-fold split, without the downsampling step for better visualization. Corresponding fit obtained with the Neural ODE for the same initial conditions and estimated phase-plane (bottom row) after 45000 training iterations. Either (\textbf{A}, \textbf{B}) using a training set of 100 deterministic trajectories, (\textbf{C},  \textbf{D}) using a training set with dynamical noise, (\textbf{E},  \textbf{F}) or a larger dataset. The red and green curves represent the nullclines of $r$ and $v$, respectively. The dots represent the initial values for each trace, while the dashed lines correspond to the Neural ODE trajectories. The neural ODE is prone to overfitting when noise is introduced in the training data, although it still preserves the overall dynamics. A larger dataset helps in recovering smoother nullclines.} 
    \label{fig:NeuralODE_montbrio}
\end{figure}


We then trained Neural ODEs using data generated by $10^4$ QIF neurons with a uniform stimulus (see \autoref{fig:NeuralODE_QIF}).
The data was partitioned using the first 400 points for training and predicting the remaining 1600 points. 
The results indicate that using derivatives dynamics, we can achieve a reliable understanding and prediction of the complex behavior of a network of spiking neurons, as illustrated in \autoref{fig:NeuralODE_QIF}. The emergent dynamics vary based on different parameter settings, resulting the stable node, stable focus, and bistable regime. See \autoref{fig:NeuralODE_loss_QIF} for the loss function in the training and test sets. 

%10, 000 iterations used for training
%We down-sampled the data with a factor of xx, hence 400 points for training and predicting 1600 points.
%% according to several values of parameters.


%We use implementation of Neural ODEs in the tvbjax package [] on QIF data $r, v$ ($10^4$ neurons, uniform stimulus) generated according to different values of parameters. Using derivatives dynamics, we are able to faithfully infer and forecast the behaviour of the underlying coupled system (\autoref{fig:NeuralODE}) in the different regimens induced by its parameters (stable node, stable focus, bistable regime, see \autoref{fig:SBI_PhaseDiagram}). %quadratic loss



\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figs/Fig10.png}
    \caption{Reconstruction and extrapolation on QIF data with various parameters and dynamic regimes, using Neural ODEs. (\textbf{A}) Stable nodes, (\textbf{B}, \textbf{C}) stable foci, (\textbf{D}, \textbf{E}) bistability. The grey dots represent sparse (down-sampled) observations, blue lines represent predictions (400 points), and orange lines represent extrapolations (1600 points). We can see that our Neural ODE has learned the system dynamics and provides accurate predictions based on data generated at the microscopic level.} 
    \label{fig:NeuralODE_QIF}
\end{figure}



\section{Discussion}

In the ever-evolving field of computational neuroscience, accurately estimating parameters that consistently govern the collective behavior of neural networks is a crucial endeavor, especially within the framework of recurrently coupled spiking neurons. In this study, we emphasize the use of mean-field (MF) theory to streamline the inference process for networks of spiking neurons. This choice is driven by the computational challenges in the calculation of the likelihood function---an essential ingredient for both frequentist and Bayesian inference methods---that becomes computationally prohibitive when attempting inference using these networks in forward modeling.


The needs and objectives of researchers in the field of computational neuroscience are as diverse as the neural systems they aim to understand. Some studies may prioritize rapid point estimation through optimization, seeking quick outcomes to inform real-time decisions \citep{Vattikonda2021, Penas2023}. On the other hand, some studies find value in exploring the full distribution of parameters, which provides a nuanced understanding of parameter uncertainty for reliable decision making \citep{Hashemi2020, Jha2022}. This paper offers a comprehensive, yet non-exhaustive, benchmarking of the state-of-the-art inference methods applied to a MF model of spiking neurons (see \autoref{fig:ODEdata}, \autoref{fig:SDEdata}, and \autoref{fig:Missingdata}). Our comparative analyses offer practical guidance, assisting researchers in selecting the most suitable method for their specific datasets and research inquiries. Note that the comparison of computational cost and accuracy in parameter estimation heavily depends on the selection of hyperparameters, such as population size in DE, warm-up phase in HMC, or the number of simulations and features in SBI. To make an unbiased assessment, we used optimal values for these hyperparameters in each algorithm, tailored to the dynamical model used in this study. Nevertheless, these optimal values may vary for different inverse problems, depending on parameter space and data dimensions, nonlinearity, sparsity and the mapping function to measurements.

While the optimization method can construct a confidence interval for the estimation based on a threshold for accepting or rejecting the estimates, the results are highly dependent on the chosen threshold value \citep{Beaumont2002, Cranmer2020}. In contrast, Bayesian inference naturally provides uncertainty quantification by placing a distribution over parameters and treating them as random variable. Following Bayes' rule, this distribution is updated with evidence from observed data to form the posterior distribution, which furnishes comprehensive information for inference and prediction. 


Our results indicated that evolutionary algorithm for solving global optimization problems, such as DE, provide rapid and accurate point estimation of the true generative parameters when there is no dynamical noise present.
Challenges arise when using optimization methods in the presence of noisy data, mainly due to the lack of an efficient form of the objective function. The selection of the objective function plays a crucial role in determining the estimation through optimization methods \citep{Hashemi2018, Svensson2012}. The error explanation with distance metrics such as RMSE is limited in accurately capturing the underlying data generation process \citep{Baldy2023}. This is because when generative parameters remain unchanged but when dynamic noise is introduced, the time-series can show large fluctuations, resulting in deviations from the observed data. In particular, the presence of noise can easily lead to unreliable estimations, increasing the risk of overfitting, where the model fits to the noise rather than capturing the true underlying relationship. Consequently, it becomes more conspicuous to conduct inference using distributions in the Bayesian framework, particularly when dealing with dynamical noise.  Diagnosing of overfitting, as shown using MAP estimation (see \autoref{fig:SDEdata} and \autoref{fig:Missingdata}), can be better understood through uncertainty quantification. 


Furthermore, Bayesian inference reveals the relationships between parameters, capturing degeneracy in the data \citep{Edelman2001, Hashemi2023}. For example, when we assess the agreement between HMC and SBI on different datasets (as shown in \autoref{fig:ReconstructedPhasePlane_and_corr}, \autoref{fig:ODE_joint_posterior}, \autoref{fig:SDE_joint_posterior}, and \autoref{fig:SDE_missingdata_joint_posterior}), it can be concluded that the persistent strong correlation between parameters $\eta$ and $J$ is inherent and not influenced by the inference procedure or model assumptions. This is in line with previous findings that have reported a strong and robust correlation between firing rates and synaptic weights across different brain states, environments and situations \citep{Buzsaki2014}. 


Using high-performance computing, model simulations can be run independently, creating a large training dataset for training deep neural density estimators in SBI approach \citep{Hashemi2023}. In contrast, HMC is limited to embarrassingly parallel execution with only independent chains on computational nodes \citep{Hashemi2021}. Moreover, when dealing with bistability in the state-space representation, HMC methods require significant computational time to detect state transitions in the latent space (see~\autoref{tab:ODEdata}, \autoref{tab:SDEdata}, and \autoref{tab:Missingdata}) or need to be augmented with generative models such as normalizing flows \citep{Hoffman2019, Gabrie2022}. On the other hand, SBI offers efficient Bayesian estimation, even without detailed knowledge of the system's state-space representation. This aligns with findings from recent studies that highlight the efficiency of SBI across various challenging inverse problems \citep{Goncalves2020, Deistler2022, Boelts2022, Boelts2023, Hashemi2023, Lavanga2023, Yalccinkaya2023, Rabuffo2023, Sorrentino2023}. By benchmarking and addressing questions such as computational cost, uncertainty quantification, inter-dependency exploration, and data availability, we conclude that SBI is more efficient than alternatives in making informed choices from microscopic states to emergent dynamics at the macro scale. 


One of the main findings of this study is the effectiveness of deep neural networks in generating probability distributions for parameters of networks of spiking neurons. This approach outperforms other computational algorithms, such as MCMC, particularly in real-world applications involving missing data in bistable systems (see \autoref{fig:Missingdata}). The effectiveness of deep generative models for inference from the mechanistic model of networks of spiking neurons is confirmed by the robustness of the estimation under significant dynamic noise (\autoref{fig:NoiseSweep}, and \autoref{fig:SBI_SDE_Istep_RV_NoiseEstimation}), as well as the precise estimation of input current (\autoref{fig:SBI_stimulus}) and the consistency with linear stability analysis (\autoref{fig:SBI_PhaseDiagram}). However, it is important to acknowledge that the use of deep generative models can result in an overestimation of uncertainty and correlations between parameters. To address this challenge, incorporating time-delay embedding is an effective remedy (\autoref{fig:ReconstructedPhasePlane_and_corr}). 


This study highlights the use of deep Neural ODEs in inferring vector fields at a macroscopic level, enabling the prediction of system dynamics from microscopic states. This approach has the potential to make interpretable predictions at larger scales from simulations at a detailed level, aiding in the prognosis and diagnosis of brain diseases. Recently, \cite{Sip2023} introduced a method using variational autoencoders (VAEs) for nonlinear dynamical system identification at the whole-brain level to infer both the neural mass model and the region- and subject-specific parameters from the functional data while respecting the known network structure. The scalability of Neural ODEs at the whole-brain network level remains to be investigated in future studies. In addition, symbolic regression applied to the outcomes from Neural ODEs may unveil closed-form equations for neural mass models, offering promising avenues for future research. This approach may lead to the discovery of concise data-driven mean-field representations of complex neural dynamics, contributing to our understanding of brain function.



In conclusion, this work highlights the improved accuracy and efficiency that deep learning techniques bring to the inference from networks of spiking neurons. It opens up exciting possibilities for future research in neural computation, where the trade-off between accuracy and uncertainty needs to be carefully considered. As we continue to explore the capabilities of Neural ODEs, this study serves as a valuable step forward in our quest to unravel the complexities of neural networks and their computational mechanisms.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section*{Information Sharing Statement}

All code is available at GitHub (\url{https://github.com/ins-amu/Inference_MFM}).


\section*{Acknowledgements}

This research has received funding from the European Union's Horizon Europe Programme under the Specific Grant Agreements No. 101147319 (EBRAINS 2.0 Project) and No. 101137289 (Virtual Brain Twin Project). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. We thank Lionel Kusch, and Spase Petkoski for fruitful discussions.


\section*{Author contributions}

Conceptualization: M.W., V.J., and M.H. Methodology: M.W., and M.H.  Software: M.W., and M.H.  Investigation: N.B.,  M.B., and M.H. Visualization: N.B.,  M.B., and M.H. Supervision:  M.H., ad V.J. Funding acquisition: V.J. Writing - original draft: N.B., and M.H. Writing - review $\&$ editing: N.B, M.B,  M.W., V.J, and M.H.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\clearpage   
%\counterwithin{figure}{section} 

\setcounter{figure}{0}

\section{Supplementary}


\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{Figs/FigS1.png}
    \renewcommand{\thefigure}{S\arabic{figure}}
    \caption{The phase-plane analysis of a mechanistic model of a network of all-to-all connected QIF neurons, using linear stability analysis. The stability of the system depends on the mean excitability ($\eta$) and synaptic weight ($J$), both normalized by the width of the input distribution ($\Delta$). (\textbf{A}) A single stable node corresponding to a low-activity state. (\textbf{B}) A single stable focus (spiral) corresponding to a high-activity state. (\textbf{C}) A bistability between low and high firing rates (stable node and stable focus, respectively). The upper section between $v$- and $r$-nulclines (in dark and light yellow, respectively) corresponds to an unstable focus. }
    \label{fig:MPR_Istep_phaseplanes}
\end{figure}





\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{Figs/FigS2.png}
    \renewcommand{\thefigure}{S\arabic{figure}}
    \caption{Data features extracted from mean membrane potential data. Additional data features that are not represented on this diagram include skewness and kurtosis.}
    \label{fig:DataFeatures}
\end{figure}






\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{Figs/FigS3.png}
    \renewcommand{\thefigure}{S\arabic{figure}}
    \caption{Sensitivity analysis on model parameters.  The profile likelihood is calculated using the root-mean-squared error (RMSE) between the observed and generated data, considering either a single parameter (represented by black curves) or multiple parameters to vary (represented by colored surfaces), while keeping the other parameters fixed. The true parameters are shown in gray, represented by a dashed vertical line (for a single parameter) or 2D coordinates (for multiple parameters).  The value of the Hessian matrix at the global minimum is displayed in the bar plot.}
    \label{fig:SensitivityAnalysis}
\end{figure}






\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{Figs/FigS4.png}
    \renewcommand{\thefigure}{S\arabic{figure}}
            \caption{Paired posterior samples of model parameters inferred from deterministic data, using (\textbf{A}) HMC and (\textbf{B}) SBI. Samples do not exhibit significant pair-wise correlation in parameter couples ($\Delta$, $\eta$) and ($\Delta$, $J$). HMC manifests high linear correlation in sampling from joint parameters ($\eta$, $J$). In terms of uncertainty quantification, HMC offers a more informative posterior distribution compared to SBI. In terms of computational cost, SBI is approximately 60 orders of magnitude faster than HMC.}
    \label{fig:ODE_joint_posterior}
\end{figure}



\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figs/FigS5.png}
    \renewcommand{\thefigure}{S\arabic{figure}}
    \caption{Paired posterior samples of model parameters inferred from noisy data, using (\textbf{A}) HMC and (\textbf{B}) SBI. Both algorithms provide uncorrelated samples for parameter couples ($\Delta$, $\eta$) and ($\Delta$, $J$) but exhibit the same level of strong linear correlation in sampling from joint parameters ($\eta$, $J$). Compared to HMC, SBI generates posteriors that more tightly center around the ground-truth parameters. Moreover, SBI remains approximately 8 orders of magnitude faster than HMC.}
    \label{fig:SDE_joint_posterior}
\end{figure}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\linewidth]{Figs/FigS6.png}
    \renewcommand{\thefigure}{S\arabic{figure}}
    \caption{Posterior z-scores versus shrinkage, as a diagnostic for the reliability of Bayesian inference. An ideal Bayesian inference yields small z-scores (indicating less error) and high posterior shrinkage (indicating more contraction with respect to the prior distribution after learning from data). Therefore, their concentration lies in the right-bottom corner of the plot. When no data is missing, both HMC (red) and SBI (blue) perform near-optimally. However, SBI is significantly faster than HMC (at least 8 orders of magnitude). The posterior z-scores is defined as $z = \mathopen | \dfrac{\bar \theta-\theta^\ast}{\sigma_{post}}\mathclose|$, where $\bar \theta$ and $\theta^\ast$ are the posterior mean and the true values, respectively, whereas $\sigma_{prior}$, and $\sigma_{post}$ indicate the standard deviations of the prior and the posterior, respectively. The posterior shrinkage defined as $s= 1- \dfrac{\sigma^2_{post}}{\sigma^2_{prior}}$, where $\sigma_{prior}$, and $\sigma_{post}$ indicate the standard deviations of the prior and the posterior, respectively. 
    } 
    \label{fig:Zscores_Shrinkage_SDE}
\end{figure}



\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{Figs/FigS7.png}
    \renewcommand{\thefigure}{S\arabic{figure}}
    \caption{SBI from noisy data.  (\textbf{A}) The time-series prediction of membrane potential ($v$) and firing rate ($r$). (\textbf{B}) Estimated posteriors of parameters $\Delta$, $\eta$, $J$, and the intensity of dynamical noise $\sigma$. The true values are shown by vertical red lines. The priors and estimated posteriors using SBI are shown in green and blue colors, respectively. Given the low-dimensional data features of time-series, SBI is able to accurately and efficiently estimate the MF parameters including the noise in the system. }
    \label{fig:SBI_SDE_Istep_RV_NoiseEstimation}
\end{figure}






\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{Figs/FigS8.png}
    \renewcommand{\thefigure}{S\arabic{figure}}
    \caption{Performance of SBI as a function of the number of simulations for the training step. (\textbf{A}) The tendency of shrinkage towards one indicates that all the posteriors are well-identified. While the shrinkage of the posterior is significantly improved when increasing the number of simulations from 1k to 10k, further increasing it beyond 100k only results in a marginal improvement. (\textbf{B}) The computational cost for SBI, which includes the simulation, training, and sampling steps, increases exponentially with respect to the number of simulations. }
    \label{fig:SBI_ZandShrink}
\end{figure}



\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figs/FigS9.png}
    \renewcommand{\thefigure}{S\arabic{figure}}
    \caption{Paired posterior samples of model parameters inferred from noisy observation with missing data, using (\textbf{A}) HMC and (\textbf{B}) SBI, when only one variable (mean membrane potential $v$) is available. HMC samples deviate considerably from the true values, while SBI samples prove still reliable under such conditions, although accompanied with high correlation. Interestingly, SBI is approximately 68 orders of magnitude faster than HMC.}
    \label{fig:SDE_missingdata_joint_posterior}
\end{figure}



\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{Figs/FigS10.png}
    \renewcommand{\thefigure}{S\arabic{figure}}
    \caption{Fit to the noisy observation with missing data (i.e., when only one variable (mean membrane potential $v$) is available): observation $v$ (top, black) and hidden $r$ (grey, bottom). The time-series fit provided by the different inference algorithms is plotted in colors.}
    \label{fig:SDEmissingdata_timeseries}
\end{figure}



\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figs/FigS11.png}
    \renewcommand{\thefigure}{S\arabic{figure}}
    \caption{Exemplifying observed ($v$ in blue and $r$ in red) and predicted ($v$ in cyan and $r$ in yellow) time-series using SBI on (\textbf{A}, \textbf{B}) Deterministic data, (\textbf{C}, \textbf{D}) Stochastic data, given the input as (\textbf{E}, \textbf{F}) Step current with $I(t)=I_0 \cdot \mathbf{1}{\text{stim}}(t)$, and sinusoidal current with $I(t)=\sin (\omega_0 t) \cdot \mathbf{1}{\text{stim}}(t)$, respectively.}
    \label{fig:SBI_Timeseries_ISin_IStep}
\end{figure}



\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{Figs/FigS12.png}
    \renewcommand{\thefigure}{S\arabic{figure}}
    \caption{Loss functions for the training and test data using Neural ODEs on MF model. (\textbf{A}, \textbf{B}) For the deterministic training set of size 100, the phase-space estimation is performed at the 22000th iteration. (\textbf{C}, \textbf{D}) Training set of size 100 with dynamical noise. (\textbf{E}, \textbf{F}) Training set of size 400 with dynamical noise.  The blue line represents the training error, while the orange line represents the test error. The vertical red line indicates the iteration at which a snapshot of the training is provided in the plot below.}
    \label{fig:NeuralODEs_Fit_traces_MPR}
\end{figure}




\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{Figs/FigS13.png}
    \renewcommand{\thefigure}{S\arabic{figure}}
    \caption{The loss function by training Neural ODEs on QIF data. The train and test quadratic loss function is calculated, when the train set is made of (\textbf{A}) 300 first points and (\textbf{B}) 400 first points. In each case, $10,000$ iterations were used for training the model.}
    \label{fig:NeuralODE_loss_QIF}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage

\begin{table}[p]
    \centering
    \begin{tabular}{lrrrrl}
    %\toprule
    {Method} & \multicolumn{2}{r}{RMSE parameters} & \multicolumn{2}{r}{RMSE time-series} & Running time \\
    {} &            mean &           std &             mean & std \\
    \midrule
    DE     &    5.256098e-16 &  1.662124e-15 &     6.757684e-19 &  1.114034e-18 &       0:0:51 \\
    PSO    &    7.181840e-01 &  8.508900e-01 &     3.221500e-04 &  2.887217e-04 &       0:0:35 \\
    BO     &    6.516364e-01 &  2.919346e-01 &     5.639526e-04 &  1.182884e-04 &      0:23:14 \\
    MAP    &    4.033716e+00 &  6.842649e-02 &     4.545021e-09 &  4.865735e-10 &       0:2:29 \\
    HMC    &    1.114323e-03 &  3.530420e-04 &     3.989852e-07 &  1.493006e-08 &     98:33:13 \\
    SBI    &    1.332644e-02 &  7.094355e-03 &     2.866951e-07 &  2.034953e-07 &      1:36:21 \\
    %\bottomrule
    \end{tabular}
\caption{Benchmark on deterministic synthetic data.}
\label{tab:ODEdata}
\end{table}




\begin{table}[p]
    \centering
    \begin{tabular}{lrrrrl}
    %\toprule
    {Method} & \multicolumn{2}{r}{RMSE parameters} & \multicolumn{2}{r}{RMSE time-series} & Running time \\
    {} &            mean &       std &             mean & std \\
    \midrule
    DE     &        0.672351 &  0.617302 &         0.000533 &  9.850079e-05 &       0:2:50 \\
    PSO    &        1.362540 &  0.852908 &         0.000717 &  1.116449e-04 &        0:1:8 \\
    BO     &        1.021078 &  0.519784 &         0.000888 &  8.232933e-05 &      0:15:36 \\
    MAP    &        3.883882 &  0.302616 &         0.000250 &  1.228436e-04 &       0:3:23 \\
    HMC    &        0.121416 &  0.081035 &         0.000004 &  1.427944e-08 &     15:56:28 \\
    SBI    &        0.052948 &  0.024310 &         0.000002 &  2.178518e-07 &      1:47:37 \\
    %\bottomrule
\end{tabular}
\caption{Benchmark on stochastic synthetic data.}
\label{tab:SDEdata}
\end{table}





\begin{table}[p]
    \centering
    \begin{tabular}{lrrrrl}
    %\toprule
    {Method} & \multicolumn{2}{r}{RMSE parameters} & \multicolumn{2}{r}{RMSE time-series} & Running time \\
    {} &            mean &       std &             mean & std \\
    \midrule
    DE & 0.426 & 0.388 & 0.218 & 0.102 & 0:4:7 \\
    PSO & 1.495 & 0.720 & 0.406 & 0.054 & 0:1:0 \\
    BO & 0.716 & 0.407 & 0.384 & 0.050 & 0:17:52 \\
    MAP & 3.539 & 0.029 & 1.838 & 0.185 & 0:0:30 \\
    HMC & 3.181 & 1.271 & 0.545 & 0.179 & 100:6:57 \\
    SBI & 0.427 & 0.296 & 0.609 & 0.213 & 1:27:34 \\
    %\bottomrule
    \end{tabular}
\caption{Benchmark on missing data. }
\label{tab:Missingdata}
\end{table}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \includemedia[width=0.6\linewidth,height=0.6\linewidth,activate=pageopen,
% passcontext,
% transparent,
% addresource={Videos/Movie1_MPR_ODE_HMC_RV.mp4},
% flashvars={source=Videos/Movie1_MPR_ODE_HMC_RV.mp4}
% ]{\includegraphics[width=0.6\linewidth]{penguins}}{VPlayer.swf}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\newpage
%plain
\bibliographystyle{elsarticle-harv}
\bibliography{Refs_MFM}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
